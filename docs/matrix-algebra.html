<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Matrix Algebra</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
   
  <!-- CSS added by filter 'toc-css.lua' for TOC hovering to the side -->
  <style>
  body {
    padding-left: 1cm;
    padding-right: 1cm;
    transition: 0.5s;
  }
  nav {
    width: 1em;
    margin-left: -1cm;
    font-size: smaller;
    color: grey;
    transition: 0.5s;
    float: left;
    position: fixed;
    top: 0;
    bottom: 0;
    white-space: nowrap; 
    overflow: hidden;
    overflow-y: scroll;
    transition: 0.5s;
  }
  nav::-webkit-scrollbar {
    display: none;
  }
  nav a, nav a:visited {
    color: grey;
  }
  nav h2:before {
    content: "≡ ";
    font-size: 150%;
  }
  nav h2:after {
    content: " ◂";
  }
  nav li {
    margin-left: 1em;
    white-space: nowrap; 
    overflow: hidden;
    text-overflow: ellipsis;
  }
  nav li > a:not(:only-child):before {
    content: "▸ ";
  }
  nav li > a:only-child {
    margin-left: 0.75em;
  }
  nav li li {
    margin-left: 1em;
  }
  nav li li li {
    margin-left: 0.5em;
    font-size: smaller;
  }
  nav ul li ul  {
    visibility: hidden;
    display: none;
    margin-top: 0.2em;
    margin-bottom: 0.2em;
    transition: 0.5s;
  }
  .paddingleft {
    padding-left: 9cm;
    transition: 0.5s;
  }
  .navside {
    width: 7cm;
    margin-left: -8.5cm;
    padding-right: 1cm;
    transition: 0.5s;
  }
  .navside h2:after {
    content: " ▸";
  }
  .navshown {
    width: 50%;
    transition: 0.5s;
    background-color: rgba(255, 255, 255, 0.95);
  }
  .subShow > ul {
    visibility: visible;
    display: block;
    transition: 0.5s;
    margin-left: -1em;
  }
  .subShow > a:not(:only-child):before {
    content: "▾ ";
  }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Matrix Algebra</h1>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul>
<li><a href="#matrices" id="toc-matrices">Matrices</a>
<ul>
<li><a href="#definition-of-matrices"
id="toc-definition-of-matrices">Definition of Matrices</a></li>
<li><a href="#adding-and-multiplying-matrices"
id="toc-adding-and-multiplying-matrices">Adding and Multiplying
Matrices</a></li>
<li><a href="#special-matrices" id="toc-special-matrices">Special
Matrices</a></li>
<li><a href="#matrix-transpose" id="toc-matrix-transpose">Matrix
Transpose</a></li>
<li><a href="#inner-outer-products" id="toc-inner-outer-products">Inner
&amp; Outer Products</a></li>
<li><a href="#matrix-inverse" id="toc-matrix-inverse">Matrix
Inverse</a></li>
<li><a href="#orthogonal-matrices"
id="toc-orthogonal-matrices">Orthogonal Matrices</a>
<ul>
<li><a href="#rotation-matrices" id="toc-rotation-matrices">Rotation
Matrices</a></li>
<li><a href="#permutation-matrices"
id="toc-permutation-matrices">Permutation Matrices</a></li>
</ul></li>
</ul></li>
<li><a href="#systems-of-linear-equations"
id="toc-systems-of-linear-equations">Systems of Linear Equations</a>
<ul>
<li><a href="#gaussian-elimination"
id="toc-gaussian-elimination">Gaussian Elimination</a></li>
<li><a href="#reduced-row-echelon-form"
id="toc-reduced-row-echelon-form">Reduced Row Echelon Form</a></li>
<li><a href="#calculating-inverses"
id="toc-calculating-inverses">Calculating Inverses</a></li>
<li><a href="#elementary-matrices"
id="toc-elementary-matrices">Elementary Matrices</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</nav>
<p><a href="index.html">to index</a></p>
<h1 id="matrices">Matrices</h1>
<h2 id="definition-of-matrices">Definition of Matrices</h2>
<p><strong>Matrices</strong> are a rectangular array of numbers,
symbols, and/or expressions. Their dimensions are represented by <span
class="math inline">\(m \times n\)</span> where <span
class="math inline">\(m\)</span> is the number of rows and <span
class="math inline">\(n\)</span> is the number of columns. Eq 1 is a
matrix of dimensions <span class="math inline">\(m \times
n\)</span>.</p>
<p><span id="eq:mat_def"><span class="math display">\[ \rm{A} =
\begin{pmatrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn}
\end{pmatrix} \qquad{(1)}\]</span></span></p>
<p><strong>Vectors</strong> are matrices that have either <span
class="math inline">\(m\)</span> or <span
class="math inline">\(n\)</span> equal to <span
class="math inline">\(1\)</span>. They can either be a row matrix (<span
class="math inline">\(m=1\)</span>) or a column matrix (<span
class="math inline">\(n=1\)</span>).</p>
<p><span class="math display">\[ \rm{B} = \begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ \rm{C} = \begin{pmatrix}
c_1 &amp; c_2 &amp; \dots &amp; c_n
\end{pmatrix} \]</span></p>
<p>In a <strong>diagonal matrix</strong>, the diagonal elements are
nonzero while all other elements are zero.</p>
<p><span class="math display">\[ \rm{D} = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ \rm{E} = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix} \]</span></p>
<h2 id="adding-and-multiplying-matrices">Adding and Multiplying
Matrices</h2>
<p><strong>Adding</strong> matrices is as easy as Eq 2. The addends need
to have the same dimensions.</p>
<p><span id="eq:add_mat"><span class="math display">\[ \begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix} +
\begin{pmatrix}
e &amp; f \\
g &amp; h
\end{pmatrix} =
\begin{pmatrix}
a+e &amp; b+f \\
c+g &amp; d+h
\end{pmatrix}\qquad{(2)}\]</span></span></p>
<p>Matrices can be multiplied by a <strong>scalar</strong>.</p>
<p><span class="math display">\[ k \begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix} =
\begin{pmatrix}
ka &amp; kb \\
kc &amp; kd
\end{pmatrix}\]</span></p>
<p><strong>Multiplying</strong> two matrices are a bit complicated. In
fact, only the combination of <span class="math inline">\((m \times n)(n
\times p)\)</span> is possible and the dimensions of the product is
<span class="math inline">\((m \times p)\)</span>. The rows of the first
matrix and the columns of the second matrix each multiplied and the
products are added.</p>
<p><span id="eq:prod_mat"><span class="math display">\[ \begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix}
\begin{pmatrix}
e &amp; f \\
g &amp; h
\end{pmatrix} =
\begin{pmatrix}
ae + bg &amp; af + bh \\
ce + dg &amp; cf + dh
\end{pmatrix}\qquad{(3)}\]</span></span></p>
<p>A general formula for an element of the matrix <span
class="math inline">\(C\)</span>, where <span
class="math inline">\(C=AB\)</span>, is</p>
<p><span id="eq:prod_elem"><span class="math display">\[ c_{ij} =
\sum_{k=1}^n a_{ik}b_{kj} \qquad{(4)}\]</span></span></p>
<p>To prove <span class="math inline">\(\rm{A(BC)=(AB)C}\)</span>, where
<span class="math inline">\(\rm{A}\)</span> is an <span
class="math inline">\(m \times n\)</span> matrix, <span
class="math inline">\(\rm{B}\)</span> is an <span
class="math inline">\(n \times p\)</span> matrix and <span
class="math inline">\(\rm{C}\)</span> is an <span
class="math inline">\(p \times q\)</span> matrix; we utilize Eq 4.
Therefore matrix multiplication is associative.</p>
<p><span class="math display">\[ \begin{align}
\rm{A(BC)} &amp;= \rm{(AB)C} \\
[\rm{A(BC)}]_{ij} &amp;= [\rm{(AB)C}]_{ij} \\
\sum_{k=1}^{n} a_{ik} [\rm{BC}]_{kj} &amp;= \sum_{l=1}^{p}
\rm{[AB]}_{il} c_{lj} \\
\sum_{k=1}^{n} \sum_{l=1}^{p} a_{lk}b_{kl}c_{lj} &amp;= \sum_{l=1}^{p}
\sum_{k=1}^{n} a_{ik}b_{kl}c_{lj}
\end{align} \]</span></p>
<h2 id="special-matrices">Special Matrices</h2>
<ul>
<li><em>Zero Matrix</em>: <span class="math inline">\(m \times
n\)</span></li>
</ul>
<p><span class="math display">\[ 0 = \begin{pmatrix}
0 &amp; 0 \\
0 &amp; 0
\end{pmatrix} \]</span></p>
<ul>
<li><em>Identity Matrix</em>: <span class="math inline">\(n \times
n\)</span></li>
</ul>
<p><span class="math display">\[ I = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ AI = A = IA \]</span></p>
<ul>
<li><em>Diagonal Matrix</em></li>
</ul>
<p><span class="math display">\[ D = \begin{pmatrix}
d_1 &amp; 0 &amp; 0 \\
0 &amp; d_2 &amp; 0 \\
0 &amp; 0 &amp; d_3
\end{pmatrix} \]</span></p>
<ul>
<li><em>Banded Matrix</em>, e.g. tridiagonal</li>
</ul>
<p><span class="math display">\[ \begin{pmatrix}
d_1 &amp; a_1 &amp; 0 \\
b_1 &amp; d_2 &amp; a_2 \\
0 &amp; b_2 &amp; d_3
\end{pmatrix} \]</span></p>
<ul>
<li><em>Upper and Lower Triangular Matrices</em></li>
</ul>
<p><span class="math display">\[ U = \begin{pmatrix}
a &amp; b &amp; c \\
0 &amp; d &amp; e \\
0 &amp; 0 &amp; f
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ L = \begin{pmatrix}
a &amp; 0 &amp; 0 \\
b &amp; c &amp; 0 \\
d &amp; e &amp; f
\end{pmatrix} \]</span></p>
<h2 id="matrix-transpose">Matrix Transpose</h2>
<p>The transpose of a matrix is its reflection about the diagonal. If a
matrix has dimensions <span class="math inline">\(m \times n\)</span>
then its transpose should have dimensions of <span
class="math inline">\(n \times m\)</span>. A general representation of a
transpose is shown in Eq 5. An example is shown in Eq 6.</p>
<p><span id="eq:mat_trans"><span class="math display">\[ \rm{A} =
\begin{pmatrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn}
\end{pmatrix} \qquad \rm{A}^{\rm{T}} = \begin{pmatrix}
a_{11} &amp; a_{21} &amp; \dots &amp; a_{m1} \\
a_{12} &amp; a_{22} &amp; \dots &amp; a_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{1n} &amp; a_{2n} &amp; \dots &amp; a_{mn}
\end{pmatrix} \qquad{(5)}\]</span></span></p>
<p><span id="eq:trans_example"><span class="math display">\[
\begin{pmatrix}
a &amp; b &amp; c \\
d &amp; e &amp; f
\end{pmatrix}^{\rm{T}} = \begin{pmatrix}
a &amp; d \\
b &amp; e \\
c &amp; f
\end{pmatrix} \qquad{(6)}\]</span></span></p>
<p>Obvious but rather useful equations can arise from this:</p>
<p><span id="eq:trans_elem"><span class="math display">\[
a^{\rm{T}}_{ij} = a_{ji} \qquad{(7)}\]</span></span></p>
<p><span id="eq:trans_trans"><span class="math display">\[
(\rm{A}^{\rm{T}})^{\rm{T}} = \rm{A} \qquad{(8)}\]</span></span></p>
<p><span id="eq:trans_sum"><span class="math display">\[ (\rm{A} +
\rm{B})^{\rm{T}} = \rm{A}^{\rm{T}} + \rm{B}^{\rm{T}}
\qquad{(9)}\]</span></span></p>
<p>A not so obvious fact is the transpose of the product of two matrices
which is given by Eq 10 which is proven subsequently in Eq 11.</p>
<p><span id="eq:trans_prod"><span class="math display">\[
(\rm{AB})^{\rm{T}} = \rm{B}^{\rm{T}}\rm{A}^{\rm{T}}
\qquad{(10)}\]</span></span></p>
<p><span id="eq:trans_prod_proof"><span class="math display">\[
\begin{align}
\left[(\rm{AB})^{\rm{T}}\right]_{ij} &amp;=
\left[\rm{B}^{\rm{T}}\rm{A}^{\rm{T}}\right]_{ij} \\
[\rm{AB}]_{ji} &amp;= \sum_{k=1}^{p} b^{\rm{T}}_{ik}a^{\rm{T}}_{kj} \\
\sum_{k=1}^{p} a_{jk}b_{ki} &amp;= \sum_{k=1}^{p} a_{jk}b_{ki}
\end{align} \qquad{(11)}\]</span></span></p>
<p>Any square matrix, <span class="math inline">\(\rm{A}\)</span> can be
expressed as the sum of a symmetric and a skew-symmetric matrix as shown
in Eq 12.</p>
<p><span id="eq:square_trans"><span class="math display">\[
\begin{gather}
\rm{A} = \begin{pmatrix}
a &amp; b &amp; c\\
d &amp; e &amp; f\\
g &amp; h &amp; i\\
\end{pmatrix} \qquad \rm{A}^{\rm{T}} = \begin{pmatrix}
a &amp; d &amp; g\\
b &amp; e &amp; h\\
c &amp; f &amp; i\\
\end{pmatrix} \\
\rm{A} + \rm{A}^{\rm{T}} = \begin{pmatrix}
2a &amp; b+d &amp; c+g\\
b+d &amp; 2e &amp; f+h\\
c+g &amp; f+h &amp; 2i\\
\end{pmatrix}\\
\rm{A} - \rm{A}^{\rm{T}} = \begin{pmatrix}
0 &amp; b-d &amp; c-g\\
d-b &amp; 0 &amp; f-h\\
g-c &amp; h-f &amp; 0\\
\end{pmatrix}\\
\rm{A} = \frac{1}{2} \left(\rm{A} + \rm{A}^{\rm{T}} + \rm{A} -
\rm{A}^{\rm{T}}\right)
\end{gather} \qquad{(12)}\]</span></span></p>
<p>The resulting matrix of <span
class="math inline">\(\rm{A}^{\rm{T}}\rm{A}\)</span> is symmetrical, as
proven in Eq 13 since <span class="math inline">\([AB]^{\rm{T}} =
B^{\rm{T}} A^{\rm{T}}\)</span> and that <span
class="math inline">\((A^{\rm{T}})^{\rm{T}} = A\)</span>.</p>
<p><span id="eq:sym_trans"><span class="math display">\[
\left[\rm{A}^{\rm{T}}\rm{A}\right]^{\rm{T}} = \rm{A}^{\rm{T}}\rm{A}
\qquad{(13)}\]</span></span></p>
<h2 id="inner-outer-products">Inner &amp; Outer Products</h2>
<p>The <strong>inner product</strong> is also called as the <strong>dot
product</strong>. It is represented in Eq 14. A concrete example is
shown in Eq 15</p>
<p><span id="eq:inner_prod"><span class="math display">\[ \rm{u} \cdot
\rm{v} = \rm{u}^{\rm{T}}\rm{v} \qquad{(14)}\]</span></span></p>
<p><span class="math display">\[ \rm{u} = \begin{pmatrix}
u_1\\
u_2\\
u_3\\
\end{pmatrix} \qquad \rm{v} = \begin{pmatrix}
v_1\\
v_2\\
v_3\\
\end{pmatrix} \]</span> <span id="eq:inner_prod_eg"><span
class="math display">\[ \rm{u}^{\rm{T}}v = \begin{pmatrix}
u_1 &amp; u_2 &amp; u_3\\
\end{pmatrix} \begin{pmatrix}
v_1\\
v_2\\
v_3\\
\end{pmatrix} = u_1v_1 + u_2v_2 + u_3v_3
\qquad{(15)}\]</span></span></p>
<p>If the inner product of two vectors are equal to zero, then the two
vectors are said to be <strong>orthogonal</strong>.</p>
<p><span id="eq:orthogonal"><span class="math display">\[
\rm{u}^{\rm{T}}\rm{v} = 0 \qquad{(16)}\]</span></span></p>
<p>The <strong>norm</strong> of a vector is written as in Eq 17, it is
the length of the vector; a concrete example is shown in Eq 18. It is
said that the vector <span class="math inline">\(\rm{u}\)</span> is
<strong>normalized</strong> if <span class="math inline">\(\left\lVert
\rm{u} \right\rVert= 1\)</span>. If two vectors are orthogonal and they
are normalized, then they are said to be
<strong>orthonormal</strong>.</p>
<p><span id="eq:norm"><span class="math display">\[ \left\lVert \rm{u}
\right\rVert = \sqrt{\rm{u}^{\rm{T}}\rm{u}}
\qquad{(17)}\]</span></span></p>
<p><span id="eq:norm_eg"><span class="math display">\[ \left\lVert
\rm{u} \right\rVert = \sqrt{u_1^2 + u_2^2 + u_3^2}
\qquad{(18)}\]</span></span></p>
<p>The <strong>outer product</strong> is the counterpart of the inner
product. It is shown in Eq 19. A concrete example is shown in Eq 20.</p>
<p><span id="eq:outer_prod"><span class="math display">\[ \rm{u} \otimes
\rm{v} = \rm{u}\rm{v}^{\rm{T}} \qquad{(19)}\]</span></span></p>
<p><span id="eq:outer_prod_eg"><span class="math display">\[
\rm{u}\rm{v}^{\rm{T}} = \begin{pmatrix}
u_1\\
u_2\\
u_3\\
\end{pmatrix} \begin{pmatrix}
v_1 &amp; v_2 &amp; v_3\\
\end{pmatrix} = \begin{pmatrix}
u_1v_1 &amp; u_1v_2 &amp; u_1v_3\\
u_2v_1 &amp; u_2v_2 &amp; u_2v_3\\
u_3v_1 &amp; u_3v_2 &amp; u_3v_3\\
\end{pmatrix} \qquad{(20)}\]</span></span></p>
<p>Let <span class="math inline">\(\rm{A}\)</span> be a rectangular
matrix given by</p>
<p><span class="math display">\[ \rm{A} = \begin{pmatrix}
a &amp; d\\
b &amp; e\\
c &amp; f\\
\end{pmatrix} \]</span></p>
<p>Calculating <span class="math inline">\(\rm{A}^{\rm{T}}A\)</span>
yields a symmetrical matrix in which the sum of the diagonal elements is
the sum of the squares of the elements of <span
class="math inline">\(\rm{A}\)</span>.</p>
<p><span class="math display">\[ \begin{pmatrix}
a &amp; b &amp; c\\
d &amp; e &amp; f\\
\end{pmatrix} \begin{pmatrix}
a &amp; d\\
b &amp; e\\
c &amp; f\\
\end{pmatrix} = \begin{pmatrix}
a^2 + b^2 + c^2 &amp; ad + be + cf\\
ad + be + cf &amp; d^2 + e^2 + f^2\\
\end{pmatrix} \]</span></p>
<p>The sum of the diagonal elements of matrix <span
class="math inline">\(\rm{B}\)</span> is the <strong>Trace</strong> of
<span class="math inline">\(\rm{B}\)</span> written as <span
class="math inline">\(\rm{Tr}\,\rm{B}\)</span>. Eq 21 shows that the sum
of the diagonals of <span
class="math inline">\(\rm{A}^{\rm{T}}\rm{A}\)</span> is the same as the
sum of the squares of the matrix <span
class="math inline">\(\rm{A}\)</span> with dimensions <span
class="math inline">\(m \times n\)</span>.</p>
<p><span id="eq:proof_trace"><span class="math display">\[ \begin{align}
\rm{Tr}\,(\rm{A}^{\rm{T}}\rm{A}) &amp;= \sum_{j=1}^{n}
(\rm{A}^{\rm{T}}\rm{A})_{jj} \\
&amp;=\sum_{j=1}^{n} \sum_{k=1}^{m} a^{\rm{T}}_{jk}a_{kj} \\
&amp;=\sum_{j=1}^{n} \sum_{k=1}^{m} a^2_{kj}
\end{align} \qquad{(21)}\]</span></span></p>
<h2 id="matrix-inverse">Matrix Inverse</h2>
<p>Not all matrices are invertible. If <span
class="math inline">\(\rm{A}\)</span> is invertible, then it’s inverse
is written as <span class="math inline">\(\rm{A}^{-1}\)</span>. Any
matrix multiplied by its inverse is equal to the identity matrix similar
to the normal reciprocal identity as shown in Eq 22.</p>
<p><span id="eq:reciprocal"><span class="math display">\[
\rm{A}\rm{A}^{-1} = \rm{A}^{-1}\rm{A} = \rm{I}
\qquad{(22)}\]</span></span></p>
<p>Some identities are shown below showing their respective proofs.</p>
<p><span id="eq:distributive_inverse"><span class="math display">\[
(\rm{AB})^{-1} = \rm{A}^{-1}\rm{B}^{-1} \qquad{(23)}\]</span></span>
<span class="math display">\[ \begin{align}
(\rm{AB})(\rm{B}^{-1}\rm{A}^{-1}) &amp;= \rm{I}\\
(\rm{B}\rm{B}^{-1})(\rm{A}\rm{A}^{-1}) &amp;= \rm{I}\\
\rm{I}\,\rm{I} &amp;= \rm{I}
\end{align} \]</span> <span id="eq:inverse_transpose"><span
class="math display">\[ (\rm{A}^{\rm{T}})^{-1} = (\rm{A}^{-1})^{\rm{T}}
\qquad{(24)}\]</span></span> <span class="math display">\[ \begin{align}
\rm{A}^{\rm{T}}(\rm{A}^{-1})^{\rm{T}} &amp;= \rm{I}\\
(\rm{A}^{-1}\rm{A})^{\rm{T}} &amp;= \rm{I}\\
\rm{I}^{\rm{T}} &amp;= \rm{I}
\end{align} \]</span></p>
<p>To express more concretely, the inverse of a general <span
class="math inline">\(2 \times 2\)</span> matrix.</p>
<p><span class="math display">\[ \begin{gather}
\rm{A} = \begin{pmatrix}
a &amp; b\\
c &amp; d\\
\end{pmatrix} \\
\rm{A}\rm{A}^{-1} = I \\
\begin{pmatrix}
a &amp; b\\
c &amp; d\\
\end{pmatrix} \begin{pmatrix}
x_1 &amp; x_2\\
y_1 &amp; y_2\\
\end{pmatrix} = \begin{pmatrix}
1 &amp; 0\\
0 &amp; 1\\
\end{pmatrix}
\end{gather} \]</span></p>
<p>By matrix multiplication, we can create 4 linear equations two of
which are homogeneous and the others inhomogeneous. We can solve for
<span class="math inline">\(y_1\)</span> and <span
class="math inline">\(y_2\)</span> in terms of <span
class="math inline">\(x_1\)</span> and <span
class="math inline">\(x_2\)</span> respectively from the homogenous
equations and solve for. Then we solve the elements of the inverse
matrix from the inhomogenous equations.</p>
<p><span class="math display">\[ \begin{gather}
ax_1 + by_1 = 1 \\
ax_2 + by_2 = 0 \\
cx_1 + dy_1 = 0 \\
cx_2 + dy_2 = 1 \\
y_1 = -\frac{c}{d}x_1 \\
y_2 = -\frac{a}{b}x_2 \\
x_1 = \frac{d}{ad-bc} \\
y_1 = \frac{-c}{ad-bc} \\
x_2 = \frac{-b}{ad-bc} \\
y_2 = \frac{a}{ad-bc} \\
\end{gather} \]</span></p>
<p>The inverse of a general <span class="math inline">\(2 \times
2\)</span> matrix is therefore given by Eq 25. If the denominator of the
scalar is <span class="math inline">\(0\)</span> then the inverse of
<span class="math inline">\(\rm{A}\)</span> does not exist. This is
called the <strong>determinant</strong> which is represented in
Eq 26.</p>
<p><span id="eq:2x2_inverse"><span class="math display">\[ \rm{A}^{-1} =
\frac{1}{ad-bc}\begin{pmatrix}
d &amp; -b\\
-c &amp; a\\
\end{pmatrix} \qquad{(25)}\]</span></span></p>
<p><span id="eq:determinant"><span class="math display">\[ \rm{det}
\rm{A} = ad - bc \qquad{(26)}\]</span></span></p>
<p><span class="math display">\[ \text{If det A}= 0,\,\text{then}\,
\rm{A}^{-1} \,\text{does not exist.} \]</span></p>
<h2 id="orthogonal-matrices">Orthogonal Matrices</h2>
<p>The inverse of an orthogonal matrix is equal to its transpose.</p>
<p><span class="math display">\[ \begin{gather}
\rm{Q}^{-1} = {\rm{Q}}\\
\rm{Q}^{\rm{T}}\rm{Q} = \rm{Q}\rm{Q}^{\rm{T}} = I\\
\end{gather} \]</span></p>
<p>The rows and rows, and columns and columns are orthonormal to each
other.</p>
<p>The orthogonal matrix preserves the norm/lengths of a vector.</p>
<p><span class="math display">\[ \begin{align}
\left\lVert Q x \right\rVert^2 &amp;= (\rm{Q}\rm{x})^{\rm{T}}
(\rm{Q}\rm{x})\\
&amp;= \rm{x}^{\rm{T}}\rm{Q}^{\rm{T}} \rm{Q}\rm{x} \\
\left\lVert x \right\rVert^2 &amp;= \rm{x}^{\rm{T}}\rm{x}\\
\end{align} \]</span></p>
<p>The product of two orthogonal matrices is itself an orthogonal
matrix.</p>
<p><span class="math display">\[ \begin{align}
\rm{Q}\rm{R} &amp;= \rm{S}\\
(\rm{Q}\rm{R})^{\rm{T}} &amp;= \rm{S}^{\rm{T}}\\
\rm{Q}\rm{R}\rm{R}^{\rm{T}}\rm{Q}^{\rm{T}} &amp;= \rm{S}^{\rm{T}}\rm{S}
= \rm{I}\\
\rm{S}^{\rm{T}}\rm{S} &amp;= I
\end{align} \]</span></p>
<h3 id="rotation-matrices">Rotation Matrices</h3>
<p>Suppose a vector, <span class="math inline">\(v\)</span>, with
elements <span class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> has an angle of <span
class="math inline">\(\phi\)</span> above the <span
class="math inline">\(x\)</span>-axis. It is to be rotated such that it
is at an angle of <span class="math inline">\(\theta\)</span> above its
initial angle. Its new elements are now <span
class="math inline">\(x&#39;\)</span> and <span
class="math inline">\(y&#39;\)</span> but it still has a length of <span
class="math inline">\(\left\lVert v \right\rVert=r\)</span>. It is
represented symbolically as in Eq 27</p>
<p><span id="eq:vector_rotation"><span class="math display">\[
\rm{R}_\theta \begin{pmatrix}
x\\
y\\
\end{pmatrix} = \begin{pmatrix}
x&#39;\\
y&#39;\\
\end{pmatrix}\qquad{(27)}\]</span></span></p>
<p>The derivation for <span class="math inline">\(\rm{R}_\theta\)</span>
can be achieved by using some trigonometric identities.</p>
<p><span class="math display">\[ \begin{align}
x&#39; &amp;= r \cos{(\phi + \theta)}\\
   &amp;= (r\cos{\phi})\cos{\theta} - (r\sin{\theta})\sin{\theta}\\
   &amp;= \cos{\theta}x - \sin{\theta}y\\
y&#39; &amp;= r \sin{(\phi + \theta)}\\
   &amp;= (r\sin{\phi})\cos{\theta} + (r\cos{\phi})\sin{\phi}\\
   &amp;= \cos{\theta}y + \sin{\phi}x
\end{align}\]</span></p>
<p><span class="math display">\[\begin{pmatrix}
\cos{\theta} &amp; -\sin{\theta}\\
\sin{\theta} &amp; \cos{\theta}\\
\end{pmatrix} \begin{pmatrix}
x\\
y\\
\end{pmatrix} = \begin{pmatrix}
x&#39;\\
y&#39;\\
\end{pmatrix}\]</span></p>
<p>The inverse of this rotation matrix rotates the vector
counter-clockwise. This can be done by inverting <span
class="math inline">\(\theta\)</span> to <span
class="math inline">\(-\theta\)</span> resulting in Eq 28. Notice that
the inverse of a rotating matrix is also its transpose, signifying that
it is an orthogonal matrix. Try to prove yourself that <span
class="math inline">\(\rm{R}(-\theta) =
\rm{R}^{-1}(\theta)\)</span>.</p>
<p><span id="eq:counter_rotation"><span class="math display">\[
\begin{gather}
\rm{R}_\theta^{-1} = \begin{pmatrix}
\cos{\theta} &amp; \sin{\theta}\\
-\sin{\theta} &amp; \cos{\theta}\\
\end{pmatrix}
\end{gather} \qquad{(28)}\]</span></span></p>
<p>For the rotation of a three-dimensional vector around the <span
class="math inline">\(z\)</span>-axis, the <span
class="math inline">\(z\)</span> coordinate stays fixed. Solving for the
rotation matrix for an angle of <span
class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[ \begin{align}
\rm{R}_z \begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix} &amp;= \begin{bmatrix}
x&#39;\\
y&#39;\\
z&#39;\\
\end{bmatrix} \\
\rm{R}_z &amp;= \begin{bmatrix}
\cos{\theta} &amp; \sin{\theta} &amp; 0\\
-\sin{\theta} &amp; \cos{\theta} &amp; 0\\
0 &amp; 0 &amp; 1\\
\end{bmatrix}
\end{align} \]</span></p>
<h3 id="permutation-matrices">Permutation Matrices</h3>
<p>A <strong>permutation</strong> matrix <em>permutes</em> the
rows/columns of another matrix that it is multiplied to. See Eq 29 and
note that the values inside the braces are the indices of the rows or
columns of that matrix. The equation tells us that there are only two
permutations of a 2x2 matrix.</p>
<p><span id="eq:permutation_2x2"><span class="math display">\[ 2 \times
2: \qquad {1,2}, {2,1} \qquad{(29)}\]</span></span></p>
<p>The identity matrix is a permutation matrix:</p>
<p><span class="math display">\[ \begin{pmatrix}
0 &amp; 1\\
1 &amp; 0\\
\end{pmatrix} \begin{pmatrix}
a &amp; b\\
c &amp; d\\
\end{pmatrix} = \begin{pmatrix}
c &amp; d\\
a &amp; b\\
\end{pmatrix}\\
\begin{pmatrix}
a &amp; b\\
c &amp; d\\
\end{pmatrix} \begin{pmatrix}
0 &amp; 1\\
1 &amp; 0\\
\end{pmatrix} = \begin{pmatrix}
b &amp; a\\
d &amp; c\\
\end{pmatrix} \]</span></p>
<p>For a 3x3 matrix, the number of permutations is <span
class="math inline">\(3! = 6\)</span>:</p>
<p><span class="math display">\[ \begin{align}
3 \times 3:\\
\{1,\,2,\,3\},\,&amp; \{1,\,3,\,2\},\\
\{2,\,1,\,3\},\,&amp; \{2,\,3,\,1\},\\
\{3,\,1,\,2\},\,&amp; \{3,\,2,\,1\}
\end{align} \]</span></p>
<p>It does not matter whether the rows or columns are permutated.</p>
<p>Work out the inverses of a three by three matrix with the
un-permutated indices of <span class="math display">\[ {1, 2, 3}
\]</span>. You will find that only those matrices that are un-permutated
and with only one permutation are their own inverses.</p>
<p>An example matrix is: <span class="math display">\[ \begin{pmatrix}
12 &amp; 3\\
-9 &amp; 7\\
\end{pmatrix} \]</span> The determinant can that I calculated was 65,
therefore the matrix is invertible.</p>
<h1 id="systems-of-linear-equations">Systems of Linear Equations</h1>
<h2 id="gaussian-elimination">Gaussian Elimination</h2>
<p>Consider we have a system of linear equations with three equations
and three unknowns as in Eq 30. We can rewrite this system into matrices
as in Eq 31, and can be written symbolically as in Eq 32.</p>
<p><span id="eq:ex_gauss"><span class="math display">\[ \begin{align}
-3 x_1 + 2 x_2 - x_3 &amp;= 1 \\
6 x_1 -6 x_2 + 7 x_3 &amp;= -7 \\
3 x_1 - 4 x_2 + 4 x_3 &amp;= -6
\end{align} \qquad{(30)}\]</span></span></p>
<p><span id="eq:matf_gauss"><span class="math display">\[ \begin{gather}
\begin{pmatrix}
-3 &amp; 2 &amp; -1\\
6 &amp; -6 &amp; 7\\
3 &amp; -4 &amp; 4\\
\end{pmatrix} \begin{pmatrix}
x_1\\
x_2\\
x_3\\
\end{pmatrix} = \begin{pmatrix}
1\\
-7\\
-6\\
\end{pmatrix}
\end{gather} \qquad{(31)}\]</span></span></p>
<p><span id="eq:sym_gauss"><span class="math display">\[ Ax = b
\qquad{(32)}\]</span></span></p>
<p>To do <em>Gaussian Elimination</em>, we must first construct the
<em>augmented matrix</em> which is done by appending the elements of
<span class="math inline">\(b\)</span> as a column in <span
class="math inline">\(A\)</span>. See Eq 33</p>
<p><span id="eq:augmented_mat"><span class="math display">\[
\begin{pmatrix}
-3 &amp; 2 &amp; -1 &amp; -1\\
6 &amp; -6 &amp; 7 &amp; -7\\
3 &amp; -4 &amp; 4 &amp; -6\\
\end{pmatrix} \qquad{(33)}\]</span></span></p>
<p>The goal of <em>Gaussian Elimination</em> is to produce an upper
triangular matrix from this <em>augmented matrix</em>. Three operations
can be done in order to preserve the solution of the system, first is to
multiply a row by a constant, second is to interchange the order of the
rows, and last is to add the elements of one row to another row. See the
following solution:</p>
<p><span class="math display">\[ \begin{gather}
\begin{pmatrix}
-3 &amp; 2 &amp; -1 &amp; -1\\
0 &amp; -2 &amp; 5 &amp; -9\\
0 &amp; -2 &amp; 3 &amp; -7\\
\end{pmatrix}
\end{gather} \]</span></p>
<p>Here, we chose the <em>pivot-position</em> to be the element -3. We
multiply the first row by a factor of 2 and added it to the second row
to zero the element below the <em>pivot-position</em>. We also added the
first row (without any scaling) to the third row to zero its first
element as well. Notice that the <em>pivot-position</em> does not
change.</p>
<p><span class="math display">\[ \begin{gather}
\begin{pmatrix}
-3 &amp; 2 &amp; -1 &amp; -1\\
0 &amp; -2 &amp; 5 &amp; -9\\
0 &amp; 0 &amp; -2 &amp; 2\\
\end{pmatrix}
\end{gather} \]</span></p>
<p>Here, we chose the pivot point to be the second element of the second
row, which is -2. Then we multiplied the second row with -1 and added it
to the third row to make its second row zero.</p>
<p>Now that the <em>augmented matrix</em> has become an upper triangular
matrix, the resulting system of equations can easily be solved by back
subsitution:</p>
<p><span class="math display">\[ \begin{align}
-3 x_1 + 2 x_2 - x_3 &amp;= -1 \\
-2 x_2 + 5 x_3 &amp;= -9 \\
-2 x_3 &amp;= 2
\end{align} \]</span></p>
<p><span class="math display">\[ \begin{align}
x_3 &amp;= -1 \\
x_2 &amp;= -\frac{1}{2}(-5 x_3 - 9) = 2 \\
x_1 &amp;= -\frac{1}{3}(-1 -2 x_2 + x_3) = 2
\end{align} \]</span></p>
<p>In the <em>augmented matrix</em> the pivot point cannot be zero.
Interchange the rows to make it non-zero.</p>
<h2 id="reduced-row-echelon-form">Reduced Row Echelon Form</h2>
<p>The <em>Reduced Row Echelon Form</em> is a matrix where the first
nonzero element of the rows is a 1 and the values above and below this
element are zeroes. It is denoted as <span
class="math inline">\(\rm{rref}(A)\)</span>. Solving for the <em>Reduced
Row Echelon Form</em> of an example matrix.</p>
<p><span class="math display">\[ A = \begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4\\
4 &amp; 5 &amp; 6 &amp; 7\\
6 &amp; 7 &amp; 8 &amp; 9\\
\end{pmatrix} \to \begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4\\
0 &amp; -3 &amp; -6 &amp; -9\\
0 &amp; -5 &amp; -10 &amp; -15\\
\end{pmatrix} \to \begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4\\
0 &amp; 1 &amp; 2 &amp; 3\\
0 &amp; 1 &amp; 2 &amp; 3\\
\end{pmatrix} \to \begin{pmatrix}
1 &amp; 0 &amp; -1 &amp; -2\\
0 &amp; 1 &amp; 2 &amp; 3\\
0 &amp; 0 &amp; 0 &amp; 0\\
\end{pmatrix} = \rm{rref}(A) \]</span></p>
<p>The resulting matrix only has two pivot points the columns in which
they are located are called the <em>pivot columns</em>. In a <em>reduced
row echelon form</em>, the pivots become 1.</p>
<p><span class="math display">\[ B = \begin{pmatrix}
3 &amp; -7 &amp; -2 &amp; -7\\
-3 &amp; 5 &amp; 1 &amp; 5\\
6 &amp; -4 &amp; 0 &amp; 2\\
\end{pmatrix} \to \begin{pmatrix}
3 &amp; -7 &amp; -2 &amp; -7\\
0 &amp; -2 &amp; -1 &amp; -2\\
0 &amp; 5 &amp; 2 &amp; 8\\
\end{pmatrix} \to \begin{pmatrix}
3 &amp; 0 &amp; 3/2 &amp; 0\\
0 &amp; -2 &amp; -1 &amp; -2\\
0 &amp; 0 &amp; -1 &amp; 6\\
\end{pmatrix} \to \begin{pmatrix}
3 &amp; 0 &amp; 0 &amp; 9\\
0 &amp; -2 &amp; 0 &amp; -8\\
0 &amp; 0 &amp; -1 &amp; 6\\
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ \rm{rref}(B) = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 3\\
0 &amp; 1 &amp; 0 &amp; 4\\
0 &amp; 0 &amp; 1 &amp; -6\\
\end{pmatrix} \]</span></p>
<p>In the above example, there are three pivot columns.</p>
<h2 id="calculating-inverses">Calculating Inverses</h2>
<p>One useful application of reduced row echelon forms is by calculating
inverses. This can be done by creating an augmented matrix with the n by
n identity matrix as the augmentor. This is based on the fact that when
multiplying the rows of a matrix with the columns of its inverse, we get
the columns of the identity matrix, so we can solve these systems of
linear equations to find the inverse of that matrix, see Eq 34. Then we
bring the <span class="math inline">\(\rm{rref}(A)\)</span> to equal
<span class="math inline">\(I\)</span> and the resulting matrix would be
an augmentation of the identity matrix and the inverse of the original
matrix.</p>
<p><span id="eq:inverse_ech"><span class="math display">\[
\begin{gather}
A A^{-1} = I \\
A a_i^{-1} = e_i \\
i \text{ pertains to the column index}
\end{gather} \qquad{(34)}\]</span></span></p>
<p>Below is an example:</p>
<p><span class="math display">\[ \begin{align}
A &amp;= \begin{pmatrix}
3 &amp; -7 &amp; -2\\
-3 &amp; 5 &amp; 1\\
6 &amp; -4 &amp; 0\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
3 &amp; -7 &amp; -2 &amp; 1 &amp; 0 &amp; 0\\
-3 &amp; 5 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\
6 &amp; -4 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
3 &amp; -7 &amp; -2 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; -2 &amp; -1 &amp; 1 &amp; 1 &amp; 0\\
0 &amp; 10 &amp; 4 &amp; -2 &amp; 0 &amp; 1\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
3 &amp; 0 &amp; 3/2 &amp; -5/2 &amp; -7/2 &amp; 0\\
0 &amp; -2 &amp; -1 &amp; 1 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; -1 &amp; 3 &amp; 5 &amp; 1\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
3 &amp; 0 &amp; 0 &amp; 2 &amp; 4 &amp; 3/2\\
0 &amp; -2 &amp; 0 &amp; -2 &amp; -4 &amp; -1\\
0 &amp; 0 &amp; -1 &amp; 3 &amp; 5 &amp; 1\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 2/3 &amp; 4/3 &amp; 1/2\\
0 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp; 1/2\\
0 &amp; 0 &amp; 1 &amp; -3 &amp; -5 &amp; -1\\
\end{pmatrix}
\end{align} \]</span></p>
<h2 id="elementary-matrices">Elementary Matrices</h2>
<p>The method of Gaussian elimination can be represented as a series of
multiplication steps to produce an upper triangular matrix. The
multipliers to the original matrix are called <em>Elementary
Matrices</em>. Below shows an example:</p>
<p><span class="math display">\[ \begin{align}
A &amp;= \begin{pmatrix}
-3 &amp; 2 &amp; -1\\
6 &amp; -6 &amp; 7\\
3 &amp; -4 &amp; 4\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
-3 &amp; 2 &amp; -1\\
0 &amp; -2 &amp; 5\\
3 &amp; -4 &amp; 4\\
\end{pmatrix} = M_1 A \\
M_1 &amp;= \begin{pmatrix}
1 &amp; 0 &amp; 0\\
2 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 1\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
-3 &amp; 2 &amp; -1\\
0 &amp; -2 &amp; 5\\
3 &amp; -4 &amp; 4\\
\end{pmatrix} = M_2 M_1 A \\
M_2 &amp;= \begin{pmatrix}
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 1\\
\end{pmatrix} \\ &amp;\to \begin{pmatrix}
-3 &amp; -2 &amp; -1\\
0 &amp; -2 &amp; 5\\
0 &amp; 0 &amp; -2\\
\end{pmatrix} = M_3 M_2 M_1 A \\
M_3 &amp;= \begin{pmatrix}
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; -1 &amp; 1\\
\end{pmatrix} \\ M_3 M_2 M_1 A &amp;= U
\end{align} \]</span></p>
<h1 id="references">References</h1>
<p>Coursera: <em>Matrix Algebra for Engineers</em>. Chasnov</p>

<!-- Javascript added by toc-css.lua to make TOC expandable on click -->
<script>

  const b = document.querySelector("body");
  const n = document.querySelector("nav");
  const buttonsize = 20;

  // click on "toc-title" to show TOC to the side
  document.querySelector("#toc-title").addEventListener("click", function(e) {
    if (e.clientX < e.currentTarget.getBoundingClientRect().left + buttonsize) {
      n.classList.toggle("navshown");
    } else {
      b.classList.toggle("paddingleft");
      n.classList.toggle("navside");
      n.classList.remove("navshown");
    };
  });

  // by default show TOC in large window
  window.onload = function() {
    if (window.innerWidth > 1000) {
      b.classList.add("paddingleft");
      n.classList.add("navside");
    };
  };

  // show/hide TOC on resize
  window.onresize = function () {
    if (window.innerWidth > 1000) {
      b.classList.add("paddingleft");
      n.classList.add("navside");
    } else {
      b.classList.remove("paddingleft");
      n.classList.remove("navside");
    };
  };

  // show/hide subsections
  const allLis = document.querySelectorAll("nav li");

  for (const li of allLis) {
    li.addEventListener('click', function (e) {
      if (e.clientX < e.currentTarget.getBoundingClientRect().left + buttonsize) {
        li.classList.toggle('subShow');
        e.preventDefault();
      };
      if (e.clientX > e.currentTarget.getBoundingClientRect().left + 3*buttonsize) {
        n.classList.remove("navshown");
      };
    });
  };

  // show full nav on tab, hide full nav on escape
  document.addEventListener("keydown", function (e) {
    if (e.which === 27) {
      n.classList.remove("navshown");
      e.preventDefault();
    };
    if (e.which === 9) {
      n.classList.add("navshown");
      e.preventDefault();
    };
  });

  // hide full nav when clicked outside
  document.addEventListener("click", function(e) {
    if (n.classList.contains("navshown")) {
      if (!n.contains(e.target)) {
        n.classList.remove("navshown");
      };
    };
  });

</script>
</body>
</html>
